# Research Results
**Query:** Models only or
**Timestamp:** 2025-11-22T14:57:53.852522
**Summary:** Models serve as essential experimental tools, with mathematical representations forming the basis for computer implementations in various fields. This approach allows for simulations that can replicate complex biological systems and processes.

## Detailed Results

### Result 1
**Title:** Of (Biological) Models and Simulations
**URL:** https://arxiv.org/abs/2302.09558?utm_source=valyu.ai&utm_medium=referral
**Source:** valyu/valyu-arxiv

**Content:**
# **Modeling, the Only Experimental Tool**

In the most basic form, a model can be seen as the box or system between the input or stimuli, and the output or response [45]. The main goal of science is to understand and explain how the response(s) is/are a result of the input(s) [46]. Taking Monod's equation as an example, the concentration of limiting substrate is the input to calculate the biomass, and the equation itself explains the logical relationship between the concentration of limiting substrate and the biomass.

This implies that there are 3 uses of models [45]. Firstly, by knowing the input and the model, the output or response can be predicted. Secondly, by knowing the model and the required response, the input can be controlled. Lastly, by knowing the input and response, the system can be understood.

However, these 3 uses are not independent. Very often, a model has to be relatively understood before it can be used for prediction or control. If a model is not substantially understood, it often gives wrong predictions and the model has to be refined and improved upon. Scientific experiments can then be seen as a procedure to generate a set of corresponding input-output pairs on which the underlying phenomenon can be understood; thus, modeled.

Although experimentation in biology is common (as evident by the large volume of experimental publications), laboratory experimentation is both difficult and unethical in many areas of biology [47], such as epidemiology and evolution. Real-world epidemiological experiments will require the willful dissemination of infectious agents into the general public as the input to track the speed and routes of transmission as the output. Clearly, this is not ethical and should never be allowed. Hence, most of epidemiology is the collection of data on existing infections from the general public [48] for the purpose of modeling the mode of infection [49] using methods such as curve fitting techniques [50]. Only then, can outcomes of disease transmission be studied and tested in a virtual context before using it for forecasting. Similarly, studying evolution experimentally is difficult and expensive [51]. Although some experimental evolution studies have been done [52-55], they are restricted to evolution of fast-growing bacterial cells. It is impossible to study human evolution in an experimental setting, both in terms of ethics and in duration.

In these ethically and/or practically impossible situations, modeling appears to be the only viable experimental tool. Computer simulations of virtual organisms (commonly known as "digital organisms" or "artificial life") had been used instead (reviewed in [56]) and may provide some insights into human evolution [57]. Moreover, there are areas that intersect between evolution and epidemiology, such as the evolution of antibiotics resistance. There have been contradictory studies showing that antibiotics resistance can possibly revert once the specific antibiotic is dis-used [58]. Although antibiotics resistance had been studied experimentally [59], it is only performed in controlled laboratory settings as it is unacceptable to willfully induce one or more antibiotics resistance in the human or animal population at large. Hence, modeling of virtual organisms has been used as an experimental tool in attempt to break the above contradiction and find that contradictory results are caused by statistical variations in the de-evolution of antibiotics resistance as the 95% confidence interval can vary from reversion to nonreversion of antibiotics resistance [58,60].

Hence, the question of whether models are useful appears to be rather muted in situations whereby models are the only feasible instruments of study.
[]
**Published:** 2023-01-01

### Result 2
**Title:** Is a mathematical model equivalent to its computer implementation?
**URL:** https://arxiv.org/abs/2402.15364?utm_source=valyu.ai&utm_medium=referral
**Source:** valyu/valyu-arxiv

**Content:**
### I. INTRODUCTION

The use of mathematical models [1] has become increasingly commonplace in many fields, including biology, physics, economics, sociology and engineering. Mathematical models are abstract representations of systems or processes that allow researchers to study their behavior under different constraints, understand the mechanisms behind a set of observations, and, finally, to make predictions. Those predictions are then typically compared with observation in the real world. Often, the basic models are based on observations from the past.

Is a model equivalent to its computer implementation? The short answer is, of course, no. It starts by the well-known fact that any number, such as for instant 1, can only be represented by a finite amount resources on a computer (float, double). Those limitations are well known and studied and have the awareness of modelers and users. However, a detailed reflection of this matter is mandatory, because:

- (1) Reflecting on this distinction between a model and its implementation offers insight into our view on mathematical models, how we represent them and how this changes over time.
- (2) Studying the equivalences (or lack thereof) of mathematical models and their implementations can allow us to understand some core reasons for the lack of reproducibility of computational results (see, e.g., [2–4]).
- (3) The topic draws attention to recent trends and innovations in academic publishing, which shape future research.

Code submission for mathematical or computational models has become a frequent requirement of funding agencies and academic journals alike. However, in the literature discussing this requirement [5–8] the differences between model and code highlighted here, as well as the potential disadvantages of resorting solely to implemented models, are hardly discussed.

A model is formulated on the level of equations (or any mathematical language appropriate for the system at hand). It is based on explicit and implicit assumptions and mostly allows for some consistence checks or proofs, for instance a result can be only a positive number and never a negative or complex number. Mathematical models are abstract formulations derived from human thought processes. They come in the form of relations between numbers, in the form of formulas or a set of equations. Often, those equations cannot be studied fully analytically (i.e., just in its original functional form). As a consequence, the original description is mathematical, but implementations of such a model are created for the purpose of running computer simulations or evaluating the model beyond what is analytically possible.

An implicit assumption of a 'good' computer implementation is that the results are a consequence of the mathematical model and independent of the model's specific implementation. This also means that any aspect pertaining solely to the computer implementation of the model is irrelevant for the model behavior and for the results obtained with the model. Examples of such implementation details are the choice of initial conditions, the sequence, in which operations formalized in the model are executed, and the numerical recipes employed for solving or simulating the model.

In some fields of science, the strength of mathematical modeling is to provide precise quantitative predictions of some measurable quantities (see also Fig. 2). In these cases, the model stems from a theory that claims to represent this aspect of the world with maximal fidelity. The quality of an implementation based on the mathematical model, which in itself is based on some theoretical model, is then typically quantified by the closeness of the predicted quantity to the measurement quantity. Even if successful in this sense, the application to another case may lead to discrepancies, requiring a change in the implementation or its underlying model.

In other cases a qualitative agreement with observations is targeted, as in the case of modeling socioeconomic systems [9, 10] or other complex systems [11–13]. Then the claim of the modeling effort is that the model contains all the mechanistic ingredients to capture a certain (often counter-intuitive) phenomenon. In these fields of science particular importance is given to the smallest, most minimal models capturing an obverved behavior (smallest in the sense of containing the smallest number of degrees of freedom and/or the smallest number of parameters). Such 'toy models' or minimal models are a cornerstone of many applications of, e.g., statistical physics [14–16].

With this perspective paper we want to draw attention to the non-trivial and crucial relationship between a mathematical model and its computer implementation and the subtle and informative ways, in which they can be different. But we also want to emphasize some potential challenges, which come along with the otherwise commendable trend in academic publishing of requiring code submissions for mathematical models [4, 17], because we feel that the current debate at times underestimates the slight distortions of the modeling landscape that come along with it.

### II. A DETAILED ASSESSMENT

In this section we illustrate our general point about the impact of code availability for mathematical models on model diversity and provide more details about the distinction of models from their implementation.
[]
**Published:** 2024-06-18

### Result 3
**Title:** Of (Biological) Models and Simulations
**URL:** https://arxiv.org/abs/2302.09558?utm_source=valyu.ai&utm_medium=referral
**Source:** valyu/valyu-arxiv

**Content:**
In fact, a model need not be highly accurate to be useful. For example, Karr, et al. [39] constructed a whole cell metabolic model of *Mycoplasma genitalium*. Although after modeling all 525 genes and its metabolism, Karr, et al. [39] managed to achieve a correlation of 0.82 (reported as R2 of 0.68) between observed experiments and simulated results. In the strictest sense, Karr, et al. [39]'s model can be deemed to be "having room for improvements" as the correlation is not near perfect (that is, R2 > 0.95) but this did not prevent the model to be used to gain insights into various metabolic processes of *M. genitalium*. This suggests that a model only needs to be sufficiently accurate or correct in some aspects but not all. This work [39] had even been commented in the same issue of the journal as the "dawn of virtual cell biology" [40]. Such large scale models had been attempted in other organisms and had provided insights [41] even for industrial applications [42]. Larger scales of models of various single cell models had been attempted for studying the emergence of microbial ecology [43] and it is not expected for these models to be entirely highly to be useful. Hence, there is no practical reason to be hung up on high accuracy of models, especially when it is generally known that laboratory experiments may have varied degrees of reproducibility due to inaccuracies of instruments (such as, imprecision of micropipetters) and skills of the researcher (such as, not adhering to protocols down to the seconds). In many cases, the purpose of the models is to provide insights, which have to be experimentally validated, but nevertheless drive research directions.

Albert Einstein is reported to have said to the effect that "*no amount of experimentation can ever prove me right; a single experiment can prove me wrong*" [44]. Science should be worried about models without limitations, and by extension, worry about models that are always right. This will only imply that such models have not been carefully examined. Thus, all models have to be wrong in some ways to render its greatest use.

At the end of the day, science had always been in the business of proving models to be wrong and that is the basis of hypothesis testing. Personally, I had threw away more models than I am willing to count. Like a tree today is sitting on a mass grave of past plants and animals, the models that I have today are standing on a graveyard of models. Yet, all these dead or failed models, which are common place and un-publishable, taught me something about my own understanding of the field and more importantly, highlighting my mis-understanding of the field.
[]
**Published:** 2023-01-01

### Result 4
**Title:** Model Pluralism
**URL:** https://arxiv.org/abs/1909.13653?utm_source=valyu.ai&utm_medium=referral
**Source:** valyu/valyu-arxiv

**Content:**
the problem of how to compare results deriving from structurally different models is one of the most interesting questions that the debate on robustness analysis has opened to today's scientific practice and promising works are expected to come from this research area in the near future" (p. 83).

Not only is it possible to usefully think about such smaller sets of models, it is also possible to consider a larger set of models than the *family of models thesis* suggests. While Ylikoski and Aydinonat (2014) insist on a rather strict form of both similarity and offspring relationships, this is not necessary. First, it could be useful to analyse incredibly dissimilar set of models, that nevertheless stand in in a genealogical relationship. Ylikoski and Aydinonat make a substantial contribution to the literature by leading us away from the simple monist appeal of trying to analyse the epistemic contribution of single models without the consideration of the larger context in which the model operates. However, they do not go far enough and fall back into the trap of monist thinking by trying to tie the semantics of what modelers mean when they speak of 'the Schelling model' to the epistemic contribution of the model. As Weisberg (2013) himself points out, there are different levels of analysis concerning models. We can take a sociological-historical approach, an epistemological approach, or a metaphysical one. While at the heart of much of the MMM literature, the latter is an avenue without much promise of success.26 Any successful analysis of models must target sets of models, their multiplicity of functions within science, and their scientific context and history.

As such, it might be useful to idealize the family of models hypothesis towards a *population of models hypothesis*. This shifts the focus away from the semantics of what scientists mean when they refer to the success of a particular model, and towards a pragmatically justified conceptual scheme. Such a scheme is pragmatic and context-sensitive. Such a scheme would be less permissive towards the misunderstanding of the contributions particular models made. I am, however, somewhat reluctant to use of the term *population of models*, as it still may lead some to mistakenly restrict themselves to treating models as only jointly explanatory if they have common ancestry. Rather it should be treated as a useful conceptual tool to think about particular models and their epistemic contributions. As Hegselmann (2017) illustrates, Sakoda (1971) invented the checkerboard model before Schelling (1971). But even without a genealogical relationship between their models, they are clearly very similar, as Hegselmann's proof of mathematical equivalence emphasizes. While an exclusion of the Sakoda-model and its 'ancestors' might be particularly useful for the purposes of sociological or historical analysis, grouping them together within a set of models could serve a multiplicity of important epistemic purposes.27

Hence, a stronger conclusion emerges from Rodrik's (2015) description of economics. It is only together that models can illuminate the mechanisms that are truly at work in complex target systems. However, though Aydinonat (2018b) argues that his analysis only provides a descriptive account of actual economic modeling practice, it suggests an explanatory factor that has not received much philosophical scrutiny but could partly explain the confidence in scientific models across disciplines. However, I am not much interested here in the specifics of Rodrik's (2015) complete account if one could even call it that. After all, it is not meant as a "treatise on economic methodology" (Rodrik 2018, p. 276), and hence unsurprisingly faces

27 The common tendency to rationalize a single model in virtue of its success should be avoided (see Veit, Dewhurst, Dolega, Jones, Stanley, Frankish, and Dennett on "The Rationale of Rationalization", forthcoming).

<sup>26</sup> See also O'Connor and Weatherall (2016).

several methodological problems once it is treated as one.28 Instead, I am more interested in the general idea of model pluralism with a much stronger conclusion than Rodrik's call for model diversity. Rodrik (2018) may be justified in invoking Ockham's razor to "use the least number of models as possible" (p. 278), but in almost all cases this will involve more models than an abstract arm-chair analysis suggests. Even when scientists specifically talk about one particular model, they will implicitly have background models in mind that are invisible to those not embedded within the scientific practice of modelers. Philosophers who attempt to understand model-based science by analysing particular models instead of sets of models commit a fatal mistake.29
[]
**Published:** 2019-01-01

### Result 5
**Title:** Computational science: shifting the focus from tools to models v2; ref status: indexed
**URL:** https://pubmed.ncbi.nlm.nih.gov/PMC4184301?utm_source=valyu.ai&utm_medium=referral
**Source:** valyu/valyu-pubmed

**Content:**
Models do not necessarily need to be quantitative. The metabolic pathways in biochemistry are a well-known example for non-quantitative models. However, in the context of computational science, nearly all models are quantitative, as they predict numbers that are compared to the numbers obtained from the actual measurements. In the following, I will limit the discussion to quantitative models.
The models most frequently discussed in the context of scientific research are those for the systems in nature that we try to understand. However, we also use physical models to describe the instruments we use to make observations, and non-physical phenomenological models to account for the aspects that we do not understand in detail. The most common models in the last category are the statistical error models, such as the very frequently (and usually silently) made assumption that an observed value is the “real” value plus an “experimental error” described by a Gaussian probability distribution. Computational studies exploring models for systems in nature are called “simulations” and are often performed on models believed to be accurate, with the goal of obtaining information that is difficult or impossible to obtain from the observation. Simulations that include a model for the scientific instruments are often labeled as “virtual experiments”. Computational studies applying statistical models to the data are called “data analysis” and typically have the goal of determining a set of model parameters that best describe the data resulted from the observation or simulation. The arguments I present in this paper apply to all of these categories.
Many scientific models are formulated in the framework of a
*theory* which defines the general rules for a large class of models. An example is classical mechanics, which is a theory describing the dynamics of systems of point masses or finite-volume rigid bodies. Within the framework of classical mechanics, a model for a concrete system can be defined by a single function called a Hamiltonian. Theories play an important role in the most mature fields of science (e.g. physics) but are not essential for defining models. Younger disciplines, e.g. systems biology, construct models in a more
*ad hoc* fashion without a clear underlying theory. Yet another approach is the construction of models derived from several theories in a multidisciplinary setting, e.g. in climate research. For the aspects that I discuss in this article, it does not matter if a model is developed in the context of some theory.
*Computable models* are the models that are of prime interest in computational science. A computable model is a computable function, as defined in computability theory
11, whose result can be compared to data from observations. Since validation requires the comparison of concrete results with observed data, one would expect that all quantitative models in science are computable models. Surprisingly, this is not the case. In fact, most mathematical models used in science are not computable.
Consider, for example, the description of the solar system in terms of classical mechanics that goes back to Isaac Newton: a set of point masses (the sun and the planets) interacting through Newton’s law of gravitation and moving according to Newton’s laws of motion. The latter are differential equations for the positions and velocities of the celestial bodies. Together with a set of parameters obtained from observation (for example, the positions and velocities of all celestial bodies at a given moment in time), these equations determine the positions and velocities at any time in the past or the future. However, they do not provide a recipe for computing the actual numbers that could be compared to observations. An additional
*approximation* is needed to obtain a computable model. For the simplest case of a system of only two celestial bodies, an analytical solution of the differential equations can be obtained. This solution contains transcendental functions (sines and cosines), which are computable to any desired precision. However, when three or more celestial bodies are included in the model, no analytical solution is available and the differential equations must be approximated by finite difference equations
12. The development of computable approximations to the Newtonian model of celestial dynamics remains an active topic of research (see e.g.
13). More generally, one can consider the whole field of numerical analysis as dedicated to constructing computable approximations to non-computable mathematical models.
It may seem surprising that most mathematical models used in the most mature domains of science do not strictly speaking deserve the label “scientific”, because they cannot make predictions that are immediately comparable to observed data. The explanation is that computation was for a long time considered a menial task not worthy of the attention of a distinguished mathematician or scientist, who should concentrate on mathematical and logical reasoning. As Dowek
14 explains in a fascinating account of the interplay of reasoning and computation in mathematics and logic in the course of history, the important role of computation in formal reasoning has become clear only during the 20th century. While today it is generally accepted by mathematicians and logicians, most theoreticians in the natural sciences still consider computation an inferior approach to exploring scientific models, which is only used out of necessity when other techniques have failed. I suspect that this lack of interest by “real theoreticians” for the computational aspects of science might have contributed to the problems that I have outlined in the introduction. This would also explain why computational training is still largely absent from the science curricula around the world.
Scientific models can be written down in many ways: mathematical equations, diagrams, plain language, etc. The same model can be represented by different notations. For example, in principle any mathematical equation could be replaced by a verbal description. Computable models can be expressed in any Turing-complete formal language, and in particular in any of the commonly used programming languages, making them the most precise and unambiguous scientific models. The very fact that a program runs and produces results proves that the model specification is complete and unambiguous, assuming that the computing system itself (hardware, operating system, compiler, etc.) works correctly and that the programming language it is written in has clearly defined semantics (which, unfortunately, is not the case for widely used languages such as C
15. The utility of computation in the process of understanding and documenting science has been pointed out by Sussman and Wisdom
16, but is not yet widely recognized in the scientific community. A nice illustration from an engineering domain (the design of musical instruments) is given by Mairson
17, who designed a computable notation for describing the geometrical constructions that have been used for a few centuries to construct string instruments. His notation is meant to be both a set of instructions for a computer and a precise and unambiguous description for human readers.
A final important point about computable models is the importance of correctly identifying, understanding and documenting approximations. Scientists frequently make approximations to computational models without recognizing them as such, and therefore do not document these approximations in their publications. A good example is the use of finite-precision floating-point numbers in place of real numbers. Most scientists would consider this a technical necessity in implementing a model on a computer, and therefore an implementation detail of computational software. However, floating-point numbers have properties that differ significantly from real numbers (for example, addition and multiplication are non-associative), and the finite precision necessarily changes the results of the computations. Making such approximations explicit would also encourage the consideration of alternatives, e.g. the use of interval arithmetic. In general, any modification to a computer program that changes its results implies an approximation to the original computational model. This also includes techniques such as lossy compression of output data, which again are usually considered implementation details.
In summary, computational science involves working with computable scientific models, which are either constructed from first principles or more frequently as approximations to non-computable models. A publication describing a computational study should contain a full description of the models that were actually used in the computations. For the models derived as approximations, this means that the final approximation, though preceding steps in the derivation, should also be given in order to document the process. Computable models can be expressed unambiguously in a Turing-complete formal language. A suitable Turing-complete language should be the preferred form for publishing models.

**Published:** 2014-06-17

### Result 6
**Title:** Quantitative Modelling in Stem Cell Biology and Beyond: How to Make Best Use of It
**URL:** https://pubmed.ncbi.nlm.nih.gov/PMC10739548?utm_source=valyu.ai&utm_medium=referral
**Source:** valyu/valyu-pubmed

**Content:**
## Data-Free Model Analysis
Commonly, quantitative models are used in view of experimental data. The study of intrinsic properties of models from a plainly theoretical standpoint, without explicitly taking into account data, is often seen as a mere academic exercise. Nonetheless, studying the intrinsic properties of models, through mathematical and computational tools, can yield impactful insights irrespective of the particular data and can guide and boost experimental campaigns substantially. The crucial benefit of theoretical, data-free model analysis is that it can assess the intrinsic consistency of models and connections between them (that is, between hypotheses), yielding shortcuts for the modelling campaign and guiding experiments. For example, by studying models theoretically, one can:1.
   discard some hypotheses a priori, by finding that they are intrinsically not consistent, or are not consistent with the circumstances of experiments.
2.
   find that some hypotheses imply each other or contradict each other.
3.
   identify Universality classes and their predictive (ir-)relevant features.
This means that many candidate hypotheses may be redundant, be it because they are intrinsically inconsistent, or follow from/ contradict other hypothesis, or are indistinguishable from others. Needless to say that this may guide towards those experiments that are really needed, and one can identify experiments that are redundant before one starts designing them, thus saving a large amount of effort and costs. An example how hypotheses can be excluded is our study of generic models for tissue cell population dynamics that includes all types of cell fate choice dynamics, in homeostasis [40]. There, it is shown, by introducing rigorous definitions of the concepts of a stem cell type, self-renewal, and homeostasis, that in a homeostatic state, only models where self-renewing cells are at the apex of a lineage hierarchy can prevail; others can be discarded without an expensive model testing campaign. Hence, by using this knowledge, coming from purely theoretical, data-free model analysis, both a lot of modelling and experimental work can be saved. Consequently, data-free model analysis can yield highly valuable information about a biological system that can save a lot of experimental and computational work and thus a lot of money, since the theoretical work is often much cheaper to be done.
Finally, theoretical model analysis can be used to reduce the number of potential candidate hypotheses that need to be tested dramatically, by finding the possible universality classes and identifying the predictive relevant features that associate a model with a universality class. Numerically, this can be done by varying parameters and structural features randomly and test under which circumstances model predictions change and when not. If an appropriate mathematical formulation of the model is available, this can also be done by mathematically taking the limits under which the emergence of universality is suspected (for large times, system size, or close to bifurcations or phase transition points), or taking a coarse-graining process such as “renormalisation” to identify the model behaviour under large scales of time and space [41, 42]. For example, in Ref. [32••], it was shown that in homeostasis, all cell fate models can be categorised in only two universality classes (with further sub-classes when different limits of parameters are considered[Fn7]). The only predictive relevant feature is a binary characteristic: whether the number of cells which retain self-renewal potential is strictly conserved over time, or not (see Fig. [Fig1]). This also explains why model 2 in Fig. [Fig1], involving cell fate priming and reversibility, predicts the same clone size distribution as model 1: in both cases, the number of stem cells is not conserved, in model 1 via symmetric divisions; in model 2, if upon a division S → S + D, the D-cell turns into an S-cell. Hence, model 2 shares the (only) predictive relevant characteristic with model 1. Another example is the classification of models for cell differentiation via catastrophe theory [19, 43]: many different dynamical systems behave in the same way close to points where the stability of the system changes (“bifurcations”), such as cells when they differentiate. Catastrophe theory was used to show that there are only few possibilities how cells can progress through differentiation [19], which again reduces the number of candidate models to be tested.
<FG id='Fig1'>
**Figure 1**: Fig. 1
<F href='40778_2023_230_Fig1_HTML.jpg'>
Hypothesis testing via quantitative modelling, exemplified on clonal statistics. **A** Depiction of lineage tracing via Cre-Lox recombination and clonal statistics. Transgenic animals carry a GFP gene preceded by a Stop sequence that is flanked by Lox constructs. The Lox-flanked Stop sequence is removed by a Cre recombinase which is expressed upon administration of tamoxifen, and thus, GFP is expressed. The GFP label is inherited to the progeny of the initial cell, which constitutes a clone and which grows over time upon Cre recombination (centre top, (c)2014 SpringerNature. Reprinted, with permission, from [28]). The statistical distributions of clone sizes is recorded (centre bottom, data from P. H. Jones as published in [29]), yet it cannot directly distinguish between hypotheses of cell fate outcome (bottom). **B** Quantitative modelling can bridge the gap between hypotheses and data: each hypothesis represents the rules for a stochastic model of cell fate choice dynamics, which predicts the hypothesis’ expected clonal statistics. The latter can then be directly compared with the experimental data and tested (bottom, data from P. H. Jones, as published in [29]). **C**, **D** Illustration of universality. **C** Two models of stem cell fate choice in homeostasis, which differ in some features, yet predict the same clone size distribution and are thus indistinguishable through static cell lineage tracing data [30] (plot on bottom: (c)2019 SpringerNature. Reprinted, with permission, from [31]). **D** ~ 800 randomly generated cell fate models can be categorised in only two universality classes, one predicting an exponential distribution in the long term limit and the other one a normal distribution if mean clone sizes are large (plots reprinted on CC-BY license from [32••]). The two classes are distinguished by only one predictive relevant property, namely, whether the number of stem cells is strictly conserved or not
</F>
</FG>
**Published:** 2023-12-11

### Result 7
**Title:** Computational modelling for decision-making: where, why, what, who and how Modelling for decision-making
**URL:** https://pubmed.ncbi.nlm.nih.gov/PMC6030334?utm_source=valyu.ai&utm_medium=referral
**Source:** valyu/valyu-pubmed

**Content:**
5.1.### Types of model
There are a wide range of computational modelling techniques, but they differ principally along a few dimensions. Selecting particular points along these dimensions implies a set of abstractions and assumptions about the system being modelled, which in turn determines how observations are represented.
1.
   *Non-deterministic* models can deliver several possible outputs from a given set of inputs. If you run a non-deterministic model today, and then run it again tomorrow with the same inputs, you may obtain different answers.
2.
   *Deterministic* models always produce one specific output from a particular set of inputs or initial conditions. Determinism in models is often highly valued, because it allows one to make absolute assertions. However, many aspects of the physical world and human behaviours are fundamentally non-deterministic, and it may not be useful to try to model them in a deterministic way.
3.
   *Static* models have no inherent concept of time and so outputs do not change over time. For instance, spreadsheets are static models, unless they explicitly encode time as an input.
4.
   *Dynamic* models have outputs that change over time. Ordinary [9] and partial differential equations [10] are common mathematical dynamic models for representing the rate of change over time; they are widely used in engineering and environmental monitoring, and also in finance and economics. System dynamics [11] is a technique based on ordinary differential equations that is used widely in business and government when considering new policies. It is used to explore the possible effects of different policies and unanticipated consequences, as well as develop understandings of the structural source of general patterns of behaviour.
5.
   *Discrete* models represent objects or events by values that go up in steps-a series of integers or characters, for example. Common discrete models are based on sets of discrete states; for instance, transition systems [12] consist of discrete states with transitions between them.
6.
   *Continuous* models involve representations that are ‘smooth’ and ‘dense’, using real numbers, for example. Differential equations are common continuous models. It is possible to combine both discrete and continuous aspects into a single model. For instance, a model may consist of a finite number of discrete states with the rates of transition between the states being continuous.
7.
   *Stochastic* (also called probabilistic or statistical) models [13] have an inherent element of random, or uncertain, behaviour and the events are assigned probabilities. This can be viewed as a special case of a non-deterministic model in which the probabilities are known.
8.
   *Individual-based* models represent each individual explicitly. These models are useful when one needs to track each individual through a system, or individuals vary significantly in their behaviour, or together the individuals form a complex and emergent system whose behaviour cannot be derived from simple aggregation. Typical examples include social insects, extremely large telecommunications networks (including the Internet), transportation networks, and stock markets. These systems are often tackled using *agent-based* models [14], typically containing a large set of autonomous agents that each represent individuals that interact with each other based on their individual attributes and behaviours.
9.
   *Population* models collectively represent large groups of individuals and are useful when individuals do not vary and an individual-based model is not tractable. When individuals do vary, but according to a small number of attributes, a population model based on *counter-abstraction* [15] that records the number of individuals with each trait (or combinations thereof) may be suitable.
10.
   *Logic* models are statements in a formal logic, which may range from classical predicate logic [16], to temporal logics [17] for future behaviours, and probabilistic temporal logics [18] for future certainties/uncertainties.
11.
   *Automata and process algebraic* models [19,20] allow simple and elegant representations of events occurring in multiple processes that send messages to each other. The underlying languages are algebraic, which means there are laws that define how the different operators (a sequence or choice between events, for example) relate to each other.
12.
   *Black-box* models fit data and trends without revealing internal workings. Machine learning [21] is a common technique based on algorithms that, in effect, learn directly from past examples, data and experience. Machine learning is most valuable where there is little prior knowledge or intuition about how a system works, but where there is considerable available data. This opens up the possibility of making predictions about the future by extrapolating patterns in the data, in domains where that has not previously been possible. At present, the results may be difficult to interpret or explain; and the models may be robust only within relatively narrow contexts.
Common example combinations of techniques include stochastic partial differential equations and hybrid automata [22]. The latter have discrete states and transitions between them, and each state is a set of differential equations that describes the continuous behaviour that applies during that state. A drawback of some combinations is that analysis can be difficult and may be poorly supported by automated tools.

5.2.### Ensemble modelling
Ensemble modelling is an important approach to model combination that involves running two or more related (but different) models, and then combining their results into a single result or comparing them. When results within the ensemble disagree, this can contribute to an understanding of whether uncertainty is present as a result of the type of model (and so the choice of model is crucial), or exists within the system. As an example, ensembles are widely used in weather forecasting, to show the different ways in which a weather system can develop.

5.3.### Analysis
Just as there are many types and techniques, there are also different ways to ask questions and obtain answers from models. Often the questions one can ask are fundamentally linked to the modelling technique. One of the most common types of analysis is simulation, usually over a time period, often called ‘running’ the model. If the model is deterministic, there is only one simulation result; the output of a static model depends entirely on the values assumed for any input parameters. But if the model is non-deterministic (i.e. has a random element) then there are many possible answers-each time you run it you will get a different answer that reflect random elements in the choices or in the environment. If you have such a model it will require many runs to achieve a representative picture of what happens.
Another type of analysis uses logic to formulate questions and reasoning techniques to answer them. For instance, questions about the performance of a modelled telecommunications service such as *after a request for a service, is there at least a 98% probability that the service will be delivered within 2 s?* can be expressed in a probabilistic temporal logic. Automated reasoning tools such as theorem provers and model checkers can be used to derive the answer.

5.4.### Role of data
Data are observations that can provide evidence for a model. The exact role of data depends on how they were obtained, and the purpose of the model. For example, if the model aims to offer rigorous explanations or predict future outcomes of an existing system, then data are necessary to validate the model. If, on the other hand, the purpose of the model is to specify a system design, or define how an intended system is required to behave, then data are used to validate the system against the model. In other words, after the system has been implemented, one checks it behaves as it should.
There is a further role for data when we are confident about the essential structure of the model, but do not know the bounds of some parameters. In this case, data are used to fine-tune parameters such as the duration or speed of an event. In all cases, care and expert judgement about interpreting validation results is required, especially when the model has been determined mainly by data with few structural assumptions imposed by the modeller, or if the data are sparse, or when it is not possible to experiment with the deployed system. For example, air traffic systems are so crucial to modern life that one cannot experiment with various parameters-such as frequency of landings or proximity of aircraft-to comprehensively check the system against the model.

**Published:** 2018-06-20

### Result 8
**Title:** Computational modelling for decision-making: where, why, what, who and how Modelling for decision-making
**URL:** https://pubmed.ncbi.nlm.nih.gov/PMC6030334?utm_source=valyu.ai&utm_medium=referral
**Source:** valyu/valyu-pubmed

**Content:**
3.1.1.#### Prediction or forecasting.
Almost all computational models ‘predict’ in the weak sense of being able to calculate an anticipated result from a given set of variables. A stronger form of prediction goes further than this, anticipating unknown (usually future) outcomes in the observed world (some describe this as ‘forecasting’). This sort of prediction is notoriously difficult for complex systems, such as biological or social systems, and thus claiming to be able to forecast for these systems may be misleading. If we truly do not know what is going to happen, it is better to be honest about that, rather than be under a false impression that we have a workable prediction. Fitting *known* data (e.g. ‘out of sample data’) is not prediction in this sense.

3.1.2.#### Explanation or exploration of future scenarios.
Particularly when considering very complex phenomena, one needs to understand why something occurs-in other words, we need to explain it. In this context, explanation means establishing a possible causal chain, from a set-up to its consequences, in terms of the mechanisms in a model. This degree of understanding is important for managing complex systems as well as understanding when predictive models might work. With many phenomena, explanation is generally much easier than prediction-models that explain why things happen can be very useful, even if they cannot predict reliably the outcomes of particular choices. For example, a social network model may help explain the survival of diverse political attitudes but not predict this [3].

3.1.3.#### Understanding theory or designs.
This usually involves extensive testing and analysis to check behaviours and assumptions in a theory or design, especially which outcomes are produced under what conditions. Outcomes can be used to help formulate a hypothesis; but they can also be used to refute a hypothesis, by exhibiting concrete counter-examples. It is important to note that although a model has to have some meaning for it to be a model, this does not necessarily imply the outcomes tell us anything about real systems. For example, many (but not all) economic models are theoretical. They might include assumptions that people behave in a perfectly rational way, for example, or that everybody has perfect access to all information. Such models might be later developed into explanatory or predictive models but currently be only about theory.

3.1.4.#### Illustration or visualization.
Sometimes one wants an illustration or visualization to communicate ideas and a model is a good way of doing this. Such a model usually relates to a specific idea or situation, and clarity of the illustration is of over-riding importance-to help people see (possibly complex) interactions at work. Crucially, an illustration cannot be relied upon for predicting or explaining. If an idea or situation is already represented as a model (designed for another purpose) then the illustrative model might well be a simplified version of this. For example, the DICE model (dynamic integrated model of climate and the economy) is a ‘simplified analytical and empirical model that represents the economics, policy, and scientific aspects of climate change’ [4]. This is a simpler version of the RICE model [5] that is used to teach about the links between the economy and climate change.

3.1.5.#### Analogy.
Playing with models in a creative but informal manner can provide new insights. Here, the model is used as an aid to thinking, and can be very powerful in this regard. However, the danger is that people confuse a useful way of thinking about things with something that is true.
If the purpose of a model is unclear or confused, this can lead to misunderstandings or errors. To give two examples, a theoretical model might be assumed to be a good way of thinking about a system, even though this might be crucially misleading, or a model that helps establish a good explanation be relied upon as a predictive model. Making clear the purpose of a model is good practice and helps others know how to judge it and what it might be reliable for.

4.## Making and using models
Models have many technical aspects, such as data, mathematical expressions and equations, and algorithms, yet these are not sufficient for a model to be useful. To get the best out of a model, model users and commissioners must work closely with model developers throughout its creation and subsequent application.

**Published:** 2018-06-20

### Result 9
**Title:** A Lightweight Image Super-Resolution Transformer Trained on Low-Resolution Images Only
**URL:** https://arxiv.org/abs/2503.23265?utm_source=valyu.ai&utm_medium=referral
**Source:** valyu/valyu-arxiv

**Content:**
#### I. INTRODUCTION

Single-image super-resolution (SISR) models aim to construct a high-resolution (HR) output image from a corresponding low-resolution (LR) input image. Conventionally, the SISR performance is measured in terms of image similarity between an SISR HR output image reconstructed from a degraded LR version and the original HR image. Also, the test images are taken from multiple benchmark datasets. The established benchmarks such as SISR on DIV2K, Set5, Set14, BSD100, Urban100 and Manga109 were long led by convolutional neural networks (CNNs) [2], [10], [20], [22], [35], [43], but were superseded quite recently by transformer models [6], [7], [17], [21], [41].

CNNs' main component are convolutional layers applying filters across the input to capture local spatial features. By progressively aggregating spatial information in deeper layers, hierarchies of features can be learned. Transformers, originally designed for natural language processing [36], excel

1https://github.com/ifnspaml/SuperResolutionMultiscaleTraining

[_page_0_Figure_10.jpeg]: Low-resolution-only training: Overview over our proposed approach to train single-image super-resolution (SISR) models only with low-resolution (LR) data, consisting of our proposed multi-scale training method for bicubic degradation (MSTbic). It creates pseudo-LR/HR training pairs enabling supervised training of a **SwinIR** lightweight (lw) transformer model. The LR-only SISR benchmark provides only lowresolution (LR) images during training, while high-resolution (HR) test data is used for evaluation.

at modeling long-range dependencies and global context by employing self-attention, which allows to process large image regions or even the entire image at once. The self-attention mechanism can model relationships between distant pixels or image features and dynamically weights their importance. In contrast to CNNs, which rely on local convolutions and hierarchical feature extraction, transformers offer more flexibly and have a lower inductive bias induced by the architectural design. We leverage these advantages and adopt a transformerbased lightweight SwinIR [21] model in this work.

The absence of high-quality HR training images in many real-world SISR applications has led to the exploration of LRonly training methods. For the LR-only task, training data is restricted to LR images, while evaluation is still conducted on HR images, defining an unsupervised task. To still enable SR model training, some approaches [3], [26], [27], [32] generate pseudo LR/HR training pairs from the available LR images. A recent method [27] addresses microscopy images, while previous approaches [3], [32] for macroscopic images report on the LR-only SISR benchmark datasets and show interesting results that are almost competitive with supervised HR-trained models. We adapt the LR-only multi-scale training method from microscopy [27] to macroscopic images of the LR-only benchmark data under a bicubic degradation condition and consequently propose MSTbic, see Fig. 1.

So far, only CNN-based models have been reported on the LR-only SISR benchmark, while CNNs had already been outperformed by transformers on other SISR benchmarks. Still, it is not obvious whether this also applies to a transformer model under a limited data condition as present in the LR-only benchmark. As transformers inherently make less assumptions about the data structure, they need to compensate for this low inductive bias by requiring extensive training data. However, in the LR-only SISR benchmark, the amount of training information is reduced to 6.25% of the original HR pixels for a 4x SR task. In addition, training image quality is also lowered by the degradation function creating the LR images.

In this work, we first employ a transformer-based lightweight SwinIR model for LR-only SISR. Second, we adopt an LR-only training method from microscopy SISR to macroscopic data. Third, we compare it with reference methods and prove its effectiveness both for a transformer and a CNN model. Finally, we set a new state of the art for the LR-only SISR benchmark.

# II. RELATED WORK

*a) CNNs and transformers for image super-resolution:* Deep learning-based SISR approaches primarily utilized CNNs due to their exceptional ability to extract local patterns. Initially featuring simple models with few layers [10], improvements such as adding residual connections [22] or subpixel convolutions [31] led to high-performance SR models [2], [10], [20], [22], [35], [43]. Although effective, their performance is constrained by the locality of the feature extraction, imposed by the receptive field of the convolutional filters. The essence of the transformer architecture [36] is the self-attention mechanism, which allows to model long-range dependencies and capture global relationships within the input sequence. Initially developed for natural language processing [36] and later adopted to high-level vision tasks [11], transformers since became predominant in super-resolution [6], [7], [17], [21], [41]. Compared to CNNs, Transformers have less inductive bias and therefore need more training data to realize their modeling potential [6]. However, standard global selfattention scales quadratically with input size leading to high computational costs. To combat the computational demand, many approaches restrict the self-attention to local regions [6], [8], [17], [21], [42]. SwinIR [21] introduced local selfattention within shifting windows to still allow information exchange between neighboring local feature regions. As the lightweight (lw) configuration provides an efficient trade-off between performance and computational cost, we deploy the SwinIR lw in our method.
['<FG id="_page_0_Figure_10.jpeg">\n**Figure 1**: Figure 1\n<F href="_page_1_Figure_1.jpeg">\nFig. 1: Low-resolution-only training: Overview over our proposed approach to train single-image super-resolution (SISR) models only with low-resolution (LR) data, consisting of our proposed multi-scale training method for bicubic degradation (MSTbic). It creates pseudo-LR/HR training pairs enabling supervised training of a **SwinIR** lightweight (lw) transformer model. The LR-only SISR benchmark provides only lowresolution (LR) images during training, while high-resolution (HR) test data is used for evaluation.\n</F>\n</FG>']
**Published:** 2025-03-30

### Result 10
**Title:** Quantitative Modelling in Stem Cell Biology and Beyond: How to Make Best Use of It
**URL:** https://pubmed.ncbi.nlm.nih.gov/PMC10739548?utm_source=valyu.ai&utm_medium=referral
**Source:** valyu/valyu-pubmed

**Content:**
### Universality: Curse and Opportunity
*Universality* is the phenomenon that different models can sometimes generate the same predictions with respect to a certain type of data, if some quantities, like mean values or passed time, are sufficiently large [27]. Models which yield the same predictions have some common features, called “predictive relevant”, but may differ substantially in others, called “predictive irrelevant” features (notably, predictive irrelevant features may yet be biologically relevant). Models that differ only in predictive irrelevant features, i.e. yielding the same predictions, can be categorised in one “universality class”, while those that differ in predictive relevant features belong to different universality classes. This has the unfortunate consequence that hypotheses that correspond to models of the same universality class will fit the data equally well and thus cannot be distinguished when the corresponding models are tested against that data. From this also follows that a fitting model does not mean that it is the “correct” model if any of the predictive irrelevant features are biologically relevant for the posed biological question, since any other model of the same universality class, but which may differ in biologically relevant, yet predictive irrelevant features, could fit the data as well.
Universality can have several origins:1.
   *Weak convergence* [33]: for stochastic processes-which model some degree of randomness-the phenomenon of “weak convergence” means that they generate statistics that converge over time, or if mean numbers are large, to the same limiting distributions, if the predictive relevant features are the same. The most common of these universal limiting distributions is the normal distribution. There is a vast number of random numbers and stochastic processes which all produce a normal distribution and thus are of the same universality class; only few predictive relevant features must be fulfilled for this: (1) the final outcome of the process is a sum of individual steps/random quantities, and (2) the mean value and variance of each step are bounded [34]. Furthermore, the number of steps (interpreted as time steps in a stochastic process) must be large. Notably, any statistical features of individual step sizes, beyond the boundedness of mean and variance, are predictive irrelevant and do not affect outcomes, if the number of steps is large.
2.
   *Non-dimensionalisation*: in both stochastic and deterministic models, quantities and parameters contain physical units, and these units can be arbitrarily chosen. For example, instead of using “seconds” as time unit, one may want to choose the inverse of the cell division rate as time unit, whereby the cell division rate becomes trivially “one division per time unit”. This can be done with other parameters as well, which thus become predictive irrelevant. By *non-dimensionalisation*,[Fn4] several different models may actually map to the same non-dimensionalised model, with a common prediction, and those thus form the same universality class.
3.
   *Universality of critical phenomena*[Fn5]: complex systems with many interacting components may display critical phenomena, like phase transitions, (e.g. liquid to gas or liquid blood that becomes a solid blood clot). Sufficiently close to the critical points, many models that differ in some features-i.e. the predictive irrelevant features-predict the same functional behaviour of the quantities describing the collective properties of those systems [35, 36]. The predictive relevant features are usually very few and often categorical, for example, which quantities are conserved, what symmetries prevail, and whether the configurations of the system are continuous or discrete (countable in integer numbers).
As an example, consider two cell fate models in homeostasis, as depicted in Fig. [Fig1]. In model 1, a stem cell (S) divides, and upon this division, the daughter cells irreversibly choose their fate, to either remain a stem cell until the next division or to commit to differentiation (C). In model 2, cell divisions are constrained to be always asymmetric, with one cell remaining a stem cell (S) and the other one being primed for differentiation (D), while the cell types may also change independently of cell division, in a reversible way, that is, an S-cell can become a D-cell and a D-cell can reverse to become an S-cell again [30]. Despite these fundamental differences, both models predict the same clone size distribution (Fig. [Fig1], bottom). Why is this, and what are the predictive relevant features those models share? To answer these questions requires some mathematical analysis, on which we will elaborate later (see also a detailed analysis in Ref. [32••]).
Besides structural features of models, the parameters of a model can be predictive relevant or irrelevant. (Predictive) *Irrelevant parameters* are those which do not change the model predictions at all when changed under conditions where universality prevails; *relevant parameters* are those which affect the model predictions. However, a minimal set of relevant parameters does not necessarily include the plain model parameters; often, predictive relevant parameters are the product or ratio of plain model parameters rather than the parameters themselves. For example, in model 1 of Fig. [Fig1], the predicted distribution of C-cells depends only on the ratio of division rate and terminal differentiation rate, not explicitly on the individual parameters themselves[20•].[Fn6] If one has found a best set of parameter values, then doubling both the division rate and the terminal differentiation rate leads to the same best fit, and thus, the “true” best set of parameters is not identifiable [37].
But universality also provides opportunities and thus may be a desired property: if we only wish to distinguish predictive relevant features, and accept for now that we cannot distinguish the predictive irrelevant ones, we do not need to test all models of a universality class but can choose the simplest model-having the lowest number of parameters and being the easiest one to evaluate and analyse-as a representative of that class and thus simplify the whole modelling campaign substantially. Since the predictive features are often categorical, the number of universality classes is usually very small, and thus, only a small number of models, one representative of each universality class, need to be tested. Furthermore, universality is to some extent essential for model testing: models always require some degree of simplification. Universality allows simplifications, that is, negligence of predictive irrelevant features, without compromising the predictive accuracy of a hypothesis/model. Without universality, that is, if all features were predictive relevant, every simplification would lead a model to deviate in its predictions from the data, and even a reasonably “true” model-when subject to some technically necessary simplifications-would not fit the data. This is usually not desired, since simplifications, and be it just for technical reasons, are often essential to evaluate models properly.
Could we overcome the limitations posed by universality? Universality emerges in view of the type of data and the circumstances under which it is collected; other types of data or a change of experimental settings may render certain predictive irrelevant features relevant and thus distinguishable. One could therefore try to obtain richer data with more features. For example, when assessing cell fate choices, one could try to directly observe them through intra-vital live imaging, to gain the time dimension as feature of the data, and with this, further details of the cell fate choices could be distinguished. While such experiments are possible in some circumstances (for example, to observe live cell fate choices in mouse epidermis [38]), they are more expensive in terms of money and effort, more invasive, or not possible in many tissues and situations. On the other hand, universality does not always emerge: usually only in limiting cases, e.g. when experiments are run over longer time scales or when numbers (such as clone sizes) are large, properties are genuinely universal [33]. When data is collected from experiments after shorter time scales or when numbers are smaller, for example, short-term cell lineage tracing after few cell divisions [28, 39], the data, and related model outputs, are not universal, and more features could, in principle, be distinguished. However, this may lead to a trade-off one wishes to avoid: while model details are easier to distinguish for short-term data, reasonable and necessary simplifications to the model may lead to undesired deviations.
To summarise, there is no one-size-fits-all solution, and a lot of intuition is needed to balance the trade-offs between the opportunities and limitations of universality: on the one hand, one wishes to distinguish a sufficient number of features, i.e. having them predictive relevant; on the other hand, one wishes to simplify the models as much as possible, by neglecting predictive irrelevant features. Ideally, the predictive relevant features are the same as the ones relevant to the biological question; this cannot be assured, but appropriate choices of experimental settings can adapt universal features for our purposes, at least to some extent. Unfortunately, the predictive (ir-)relevant features which define the universality classes are often not known beforehand. Then, we may need to travel down the rocky route and follow the classical scientific method, according to K. Popper: come up with a set of all plausible hypotheses and test the corresponding models for all of them; reject those hypotheses which cannot be brought in accordance with the data through fitting, while those that fit (possibly more than one model) may then constitute the universality class of the “true” model. Without prior knowledge about universality classes, however, the number of candidate models to test could be extremely large and arbitrarily complex. Hence, in order to optimise a modelling approach, it is essential to gather some a priori knowledge about the universality classes and their predictive relevant features. This can only be obtained by a mathematical analysis of candidate models’ properties beforehand, as described in the following section.
<FG id='Fig1'>
**Figure 1**: Fig. 1
<F href='40778_2023_230_Fig1_HTML.jpg'>
Hypothesis testing via quantitative modelling, exemplified on clonal statistics. **A** Depiction of lineage tracing via Cre-Lox recombination and clonal statistics. Transgenic animals carry a GFP gene preceded by a Stop sequence that is flanked by Lox constructs. The Lox-flanked Stop sequence is removed by a Cre recombinase which is expressed upon administration of tamoxifen, and thus, GFP is expressed. The GFP label is inherited to the progeny of the initial cell, which constitutes a clone and which grows over time upon Cre recombination (centre top, (c)2014 SpringerNature. Reprinted, with permission, from [28]). The statistical distributions of clone sizes is recorded (centre bottom, data from P. H. Jones as published in [29]), yet it cannot directly distinguish between hypotheses of cell fate outcome (bottom). **B** Quantitative modelling can bridge the gap between hypotheses and data: each hypothesis represents the rules for a stochastic model of cell fate choice dynamics, which predicts the hypothesis’ expected clonal statistics. The latter can then be directly compared with the experimental data and tested (bottom, data from P. H. Jones, as published in [29]). **C**, **D** Illustration of universality. **C** Two models of stem cell fate choice in homeostasis, which differ in some features, yet predict the same clone size distribution and are thus indistinguishable through static cell lineage tracing data [30] (plot on bottom: (c)2019 SpringerNature. Reprinted, with permission, from [31]). **D** ~ 800 randomly generated cell fate models can be categorised in only two universality classes, one predicting an exponential distribution in the long term limit and the other one a normal distribution if mean clone sizes are large (plots reprinted on CC-BY license from [32••]). The two classes are distinguished by only one predictive relevant property, namely, whether the number of stem cells is strictly conserved or not
</F>
</FG>
**Published:** 2023-12-11
